{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.483 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "# unsloth/codellama-7b-bnb-4bit\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-2-7b-bnb-4bit\", # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GSM8K Prompts\n",
    "\n",
    "PREAMBLE = \"\"\"As an expert problem solver solve step by step the following mathematical questions.\"\"\"\n",
    "\n",
    "# The default gsm8k prompt from the CoT paper\n",
    "# https://arxiv.org/pdf/2201.11903.pdf page 35.\n",
    "\n",
    "PROMPT = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah's sister had 42. That means there were originally 32 + 42 = 74 chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She had $23 in beginning, so now she has $23 - $15 = $8. The answer is 8.\"\"\"\n",
    "\n",
    "\n",
    "# Extension of the default 8-shot prompt, page 35 in\n",
    "# https://arxiv.org/pdf/2201.11903.pdf\n",
    "# The extension is intended to improve performance on\n",
    "# more complicated gsm8k examples.\n",
    "\n",
    "EXTRA_3_SHOTS = \"\"\"As an expert problem solver solve step by step the following mathematical questions.\n",
    "\n",
    "Q: Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?\n",
    "A: Here's how to calculate Tina's earnings:\n",
    "\n",
    "**Regular Time:**\n",
    "- Hours per shift: 8 hours\n",
    "- Wage per hour: $18.00\n",
    "- Regular pay per shift: 8 hours * $18.00/hour = $144.00\n",
    "\n",
    "**Overtime:**\n",
    "- Overtime hours per shift: 10 hours - 8 hours = 2 hours\n",
    "- Overtime pay per hour: $18.00 + ($18.00 / 2) = $27.00\n",
    "- Overtime pay per shift: 2 hours * $27.00/hour = $54.00\n",
    "\n",
    "**Total per day:**\n",
    "- Regular pay + overtime pay: $144.00/shift + $54.00/shift = $198.00/day\n",
    "\n",
    "**Total for 5 days:**\n",
    "- 5 days * $198.00/day = $990.00\n",
    "\n",
    "**Therefore, Tina will make $990.00 in 5 days.** The answer is 990.\n",
    "\n",
    "Q: Abigail is trying a new recipe for a cold drink. It uses 1/4 of a cup of iced tea and 1 and 1/4 of a cup of lemonade to make one drink. If she fills a pitcher with 18 total cups of this drink, how many cups of lemonade are in the pitcher?\n",
    "A: ## Ambiguity in the Problem Statement:\n",
    "\n",
    "There is one main ambiguity in the problem statement:\n",
    "\n",
    "**Total volume vs. Number of servings:** The statement \"18 total cups of this drink\" could be interpreted in two ways:\n",
    "  * **18 cups of the combined volume:** This would mean Abigail used a total of 18 cups of liquid, including both iced tea and lemonade.\n",
    "  * **18 individual servings:** This would mean Abigail made 18 individual drinks, each containing 1/4 cup of iced tea and 1 1/4 cup of lemonade.\n",
    "\n",
    "Let us assume the interpretation \"18 cups of the combined volume\".\n",
    "\n",
    "## Solution assuming 18 cups of combined volume:\n",
    "\n",
    "**Step 1: Find the proportion of lemonade in one drink:**\n",
    "\n",
    "* Lemonade: 1 1/4 cups\n",
    "* Iced tea: 1/4 cup\n",
    "* Total: 1 1/4 + 1/4 = 1 1/2 cups\n",
    "* Lemonade proportion: (1 1/4) / (1 1/2) = 5/6\n",
    "\n",
    "**Step 2: Calculate the amount of lemonade in the pitcher:**\n",
    "\n",
    "* Total volume: 18 cups\n",
    "* Lemonade proportion: 5/6\n",
    "* Volume of lemonade: 18 * (5/6) = 15 cups\n",
    "\n",
    "Therefore, there are 15 cups of lemonade in the pitcher. The answer is 15.\n",
    "\n",
    "Q: A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\n",
    "A: Let us solve it using algebra. Let x be the number of people on the ship the monster ate in the first hundred years.\n",
    "\n",
    "The number of people on the ship eaten in the second hundred years is 2x, and in the third hundred years is 4x.\n",
    "\n",
    "Therefore, the total number of people eaten over three hundred years is x + 2x + 4x = 847.\n",
    "\n",
    "Combining like terms, we get 7x = 847.\n",
    "\n",
    "Dividing both sides by 7, we find x = 121.\n",
    "\n",
    "Therefore, there were 121 people on the ship the monster ate in the first hundred years. The answer is 121.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_number_from_text(text, prefix=\"The answer is\"):\n",
    "    \"\"\"\n",
    "    Extracts the last number from a text string that follows a given prefix.\n",
    "    Args:\n",
    "        text (str): The text from which to extract the number.\n",
    "        prefix (str): The prefix to search for before extracting the number.\n",
    "    Returns:\n",
    "        float or None: The extracted number, or None if no valid number is found.\n",
    "    \"\"\"\n",
    "    # Find the part of the text that starts with the prefix\n",
    "    match = re.search(re.escape(prefix) + r\".*\", text)\n",
    "    if match:\n",
    "        # Extract all numbers from the matched text\n",
    "        numbers = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", match.group(0))\n",
    "        if numbers:\n",
    "            # Return the last number found as a float\n",
    "            last_number = numbers[-1]\n",
    "            try:\n",
    "                return float(last_number)\n",
    "            except ValueError:\n",
    "                print(f\"Could not convert '{last_number}' to float.\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def extract_response_after_question(full_output, question):\n",
    "    \"\"\"\n",
    "    Extracts the line immediately following the question in the model's output.\n",
    "\n",
    "    Args:\n",
    "    - full_output (str): The complete output from the model.\n",
    "    - question (str): The question text used to locate the response line.\n",
    "\n",
    "    Returns:\n",
    "    - str: The line following the question line or None if not found.\n",
    "    \"\"\"\n",
    "    # Normalize line breaks\n",
    "    full_output = full_output.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    lines = full_output.split('\\n')\n",
    "\n",
    "    # Attempt to find the line containing the question\n",
    "    for i, line in enumerate(lines):\n",
    "        if question in line:\n",
    "            # Return the next line if it exists\n",
    "            if i + 1 < len(lines):\n",
    "                return lines[i + 1].strip()\n",
    "            break\n",
    "\n",
    "    return None\n",
    "\n",
    "# # Example Usage:\n",
    "# full_output = \"\"\"\n",
    "# Q: How many eggs do Janet's ducks lay?\n",
    "# A: Janet's ducks lay 16 eggs per day. She eats 3 for breakfast.\n",
    "# She bakes muffins with 4. She sells the rest for $2 per fresh duck egg.\n",
    "# So, she gets 16 * 3 - 4 * 2 = $48. The answer is $48.\n",
    "# \"\"\"\n",
    "# question = \"How many eggs do Janet's ducks lay?\"\n",
    "\n",
    "# next_line = extract_response_after_question(full_output, question)\n",
    "# print(f\"Response after the question: '{next_line}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_id 0\n",
      "Model answer: None\n",
      "Ground truth answer: 18.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 1\n",
      "Model answer: 2.5\n",
      "Ground truth answer: 3.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 2\n",
      "Model answer: 0.0\n",
      "Ground truth answer: 70000.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 3\n",
      "Model answer: 180.0\n",
      "Ground truth answer: 540.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 4\n",
      "Model answer: 40.0\n",
      "Ground truth answer: 20.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 5\n",
      "Model answer: None\n",
      "Ground truth answer: 64.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 6\n",
      "Model answer: 160.0\n",
      "Ground truth answer: 260.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 7\n",
      "Model answer: None\n",
      "Ground truth answer: 160.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 8\n",
      "Model answer: None\n",
      "Ground truth answer: 45.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 9\n",
      "Model answer: 450.0\n",
      "Ground truth answer: 460.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 10\n",
      "Model answer: None\n",
      "Ground truth answer: 366.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 11\n",
      "Model answer: 405.0\n",
      "Ground truth answer: 694.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 12\n",
      "Model answer: None\n",
      "Ground truth answer: 13.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 13\n",
      "Model answer: 2.0\n",
      "Ground truth answer: 18.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 14\n",
      "Model answer: None\n",
      "Ground truth answer: 60.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 15\n",
      "Model answer: None\n",
      "Ground truth answer: 125.0\n",
      "Correct: 0 out of 100\n",
      "========================================\n",
      "task_id 16\n",
      "Model answer: 230.0\n",
      "Ground truth answer: 230.0\n",
      "Correct: 1 out of 100\n",
      "========================================\n",
      "task_id 17\n",
      "Model answer: None\n",
      "Ground truth answer: 57500.0\n",
      "Correct: 1 out of 100\n",
      "========================================\n",
      "task_id 18\n",
      "Model answer: 48.0\n",
      "Ground truth answer: 7.0\n",
      "Correct: 1 out of 100\n",
      "========================================\n",
      "task_id 19\n",
      "Model answer: 3.0\n",
      "Ground truth answer: 6.0\n",
      "Correct: 1 out of 100\n",
      "========================================\n",
      "task_id 20\n",
      "Model answer: 16.0\n",
      "Ground truth answer: 15.0\n",
      "Correct: 1 out of 100\n",
      "========================================\n",
      "task_id 21\n",
      "Model answer: 29.0\n",
      "Ground truth answer: 14.0\n",
      "Correct: 1 out of 100\n",
      "========================================\n",
      "task_id 22\n",
      "Model answer: 7.0\n",
      "Ground truth answer: 7.0\n",
      "Correct: 2 out of 100\n",
      "========================================\n",
      "task_id 23\n",
      "Model answer: 8.0\n",
      "Ground truth answer: 8.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 24\n",
      "Model answer: 19.125\n",
      "Ground truth answer: 26.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 25\n",
      "Model answer: 1.0\n",
      "Ground truth answer: 2.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 26\n",
      "Model answer: None\n",
      "Ground truth answer: 243.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 27\n",
      "Model answer: 3600.0\n",
      "Ground truth answer: 16.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 28\n",
      "Model answer: 35.0\n",
      "Ground truth answer: 25.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 29\n",
      "Model answer: 10.0\n",
      "Ground truth answer: 104.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 30\n",
      "Model answer: 11.0\n",
      "Ground truth answer: 109.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 31\n",
      "Model answer: 100.0\n",
      "Ground truth answer: 80.0\n",
      "Correct: 3 out of 100\n",
      "========================================\n",
      "task_id 32\n",
      "Model answer: 35.0\n",
      "Ground truth answer: 35.0\n",
      "Correct: 4 out of 100\n",
      "========================================\n",
      "task_id 33\n",
      "Model answer: 80.0\n",
      "Ground truth answer: 70.0\n",
      "Correct: 4 out of 100\n",
      "========================================\n",
      "task_id 34\n",
      "Model answer: 36.0\n",
      "Ground truth answer: 23.0\n",
      "Correct: 4 out of 100\n",
      "========================================\n",
      "task_id 35\n",
      "Model answer: 9.0\n",
      "Ground truth answer: 9.0\n",
      "Correct: 5 out of 100\n",
      "========================================\n",
      "task_id 36\n",
      "Model answer: 300.0\n",
      "Ground truth answer: 75.0\n",
      "Correct: 5 out of 100\n",
      "========================================\n",
      "task_id 37\n",
      "Model answer: 2.0\n",
      "Ground truth answer: 2.0\n",
      "Correct: 6 out of 100\n",
      "========================================\n",
      "task_id 38\n",
      "Model answer: None\n",
      "Ground truth answer: 10.0\n",
      "Correct: 6 out of 100\n",
      "========================================\n",
      "task_id 39\n",
      "Model answer: None\n",
      "Ground truth answer: 18.0\n",
      "Correct: 6 out of 100\n",
      "========================================\n",
      "task_id 40\n",
      "Model answer: 8.0\n",
      "Ground truth answer: 8.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 41\n",
      "Model answer: None\n",
      "Ground truth answer: 200.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 42\n",
      "Model answer: None\n",
      "Ground truth answer: 26.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 43\n",
      "Model answer: None\n",
      "Ground truth answer: 48.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 44\n",
      "Model answer: None\n",
      "Ground truth answer: 20.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 45\n",
      "Model answer: 17.0\n",
      "Ground truth answer: 104.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 46\n",
      "Model answer: 103.0\n",
      "Ground truth answer: 163.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 47\n",
      "Model answer: 120.0\n",
      "Ground truth answer: 800.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 48\n",
      "Model answer: 24.0\n",
      "Ground truth answer: 8.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 49\n",
      "Model answer: 3.75\n",
      "Ground truth answer: 30.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 50\n",
      "Model answer: None\n",
      "Ground truth answer: 294.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 51\n",
      "Model answer: 10.0\n",
      "Ground truth answer: 5.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 52\n",
      "Model answer: None\n",
      "Ground truth answer: 15.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 53\n",
      "Model answer: None\n",
      "Ground truth answer: 40.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 54\n",
      "Model answer: 28.0\n",
      "Ground truth answer: 40.0\n",
      "Correct: 7 out of 100\n",
      "========================================\n",
      "task_id 55\n",
      "Model answer: 14.0\n",
      "Ground truth answer: 14.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 56\n",
      "Model answer: 42.0\n",
      "Ground truth answer: 3.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 57\n",
      "Model answer: 11245.0\n",
      "Ground truth answer: 83.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 58\n",
      "Model answer: None\n",
      "Ground truth answer: 57.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 59\n",
      "Model answer: 1267.0\n",
      "Ground truth answer: 187.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 60\n",
      "Model answer: 22.0\n",
      "Ground truth answer: 17.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 61\n",
      "Model answer: 1380.0\n",
      "Ground truth answer: 1430.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 62\n",
      "Model answer: 5000.0\n",
      "Ground truth answer: 25000.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 63\n",
      "Model answer: None\n",
      "Ground truth answer: 1596.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 64\n",
      "Model answer: 8.0\n",
      "Ground truth answer: 300.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 65\n",
      "Model answer: 5.0\n",
      "Ground truth answer: 36.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 66\n",
      "Model answer: 40.0\n",
      "Ground truth answer: 48.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 67\n",
      "Model answer: 200.0\n",
      "Ground truth answer: 595.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 68\n",
      "Model answer: 72.0\n",
      "Ground truth answer: 36.0\n",
      "Correct: 8 out of 100\n",
      "========================================\n",
      "task_id 69\n",
      "Model answer: 60.0\n",
      "Ground truth answer: 60.0\n",
      "Correct: 9 out of 100\n",
      "========================================\n",
      "task_id 70\n",
      "Model answer: 2925.0\n",
      "Ground truth answer: 7425.0\n",
      "Correct: 9 out of 100\n",
      "========================================\n",
      "task_id 71\n",
      "Model answer: 60.0\n",
      "Ground truth answer: 60.0\n",
      "Correct: 10 out of 100\n",
      "========================================\n",
      "task_id 72\n",
      "Model answer: 218.0\n",
      "Ground truth answer: 221.0\n",
      "Correct: 10 out of 100\n",
      "========================================\n",
      "task_id 73\n",
      "Model answer: 750.0\n",
      "Ground truth answer: 255.0\n",
      "Correct: 10 out of 100\n",
      "========================================\n",
      "task_id 74\n",
      "Model answer: None\n",
      "Ground truth answer: 88.0\n",
      "Correct: 10 out of 100\n",
      "========================================\n",
      "task_id 75\n",
      "Model answer: 0.25\n",
      "Ground truth answer: 60.0\n",
      "Correct: 10 out of 100\n",
      "========================================\n",
      "task_id 76\n",
      "Model answer: 185.0\n",
      "Ground truth answer: 5.0\n",
      "Correct: 10 out of 100\n",
      "========================================\n",
      "task_id 77\n",
      "Model answer: None\n",
      "Ground truth answer: 100.0\n",
      "Correct: 10 out of 100\n",
      "========================================\n",
      "task_id 78\n",
      "Model answer: 2.0\n",
      "Ground truth answer: 6.0\n",
      "Correct: 10 out of 100\n",
      "========================================\n",
      "task_id 79\n",
      "Model answer: 70.0\n",
      "Ground truth answer: 70.0\n",
      "Correct: 11 out of 100\n",
      "========================================\n",
      "task_id 80\n",
      "Model answer: 2.0\n",
      "Ground truth answer: 10.0\n",
      "Correct: 11 out of 100\n",
      "========================================\n",
      "task_id 81\n",
      "Model answer: -34.0\n",
      "Ground truth answer: 17.0\n",
      "Correct: 11 out of 100\n",
      "========================================\n",
      "task_id 82\n",
      "Model answer: 248.0\n",
      "Ground truth answer: 623.0\n",
      "Correct: 11 out of 100\n",
      "========================================\n",
      "task_id 83\n",
      "Model answer: 600.0\n",
      "Ground truth answer: 600.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 84\n",
      "Model answer: 22.0\n",
      "Ground truth answer: 15.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 85\n",
      "Model answer: 48.0\n",
      "Ground truth answer: 44.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 86\n",
      "Model answer: None\n",
      "Ground truth answer: 22.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 87\n",
      "Model answer: 738.0\n",
      "Ground truth answer: 9360.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 88\n",
      "Model answer: 200.0\n",
      "Ground truth answer: 8000.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 89\n",
      "Model answer: 8.0\n",
      "Ground truth answer: 24.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 90\n",
      "Model answer: 200.0\n",
      "Ground truth answer: 225.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 91\n",
      "Model answer: 15.0\n",
      "Ground truth answer: 28.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 92\n",
      "Model answer: None\n",
      "Ground truth answer: 4.0\n",
      "Correct: 12 out of 100\n",
      "========================================\n",
      "task_id 93\n",
      "Model answer: 36.0\n",
      "Ground truth answer: 36.0\n",
      "Correct: 13 out of 100\n",
      "========================================\n",
      "task_id 94\n",
      "Model answer: 120.0\n",
      "Ground truth answer: 348.0\n",
      "Correct: 13 out of 100\n",
      "========================================\n",
      "task_id 95\n",
      "Model answer: 140.0\n",
      "Ground truth answer: 40.0\n",
      "Correct: 13 out of 100\n",
      "========================================\n",
      "task_id 96\n",
      "Model answer: 3.0\n",
      "Ground truth answer: 3.0\n",
      "Correct: 14 out of 100\n",
      "========================================\n",
      "task_id 97\n",
      "Model answer: 48.0\n",
      "Ground truth answer: 12.0\n",
      "Correct: 14 out of 100\n",
      "========================================\n",
      "task_id 98\n",
      "Model answer: 30.0\n",
      "Ground truth answer: 5.0\n",
      "Correct: 14 out of 100\n",
      "========================================\n",
      "task_id 99\n",
      "Model answer: 0.0\n",
      "Ground truth answer: 58.0\n",
      "Correct: 14 out of 100\n",
      "========================================\n",
      "Final Accuracy: 0.01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "Q: {}\n",
    "A:\"\"\"\n",
    "\n",
    "# Load GSM8K dataset\n",
    "gsm8k_test = load_dataset(\"gsm8k\",\"main\", split=\"test\")\n",
    "\n",
    "# Assuming model and tokenizer are already initialized\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Helper function to encode inputs\n",
    "def prepare_input(p):\n",
    "    # print(question)\n",
    "    prompt = (PREAMBLE +'\\n\\n' + PROMPT + '\\n' +\n",
    "                 TEMPLATE.format(p))\n",
    "    return tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "# Function to decode model output\n",
    "def decode_output(output_ids):\n",
    "    return tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Manual testing loop\n",
    "all_correct = 0\n",
    "all_responses = {}\n",
    "idx = 0\n",
    "total = len(gsm8k_test)\n",
    "total = 100\n",
    "\n",
    "for task_id, problem in enumerate(gsm8k_test):\n",
    "    if idx == total:\n",
    "        break\n",
    "    \n",
    "    print(f\"task_id {task_id}\")\n",
    "\n",
    "    # Prepare the input for the model\n",
    "    input_ids = prepare_input(problem['question'])\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_new_tokens = 120)  # Adjust max_length as needed\n",
    "\n",
    "    response = decode_output(output_ids[0])\n",
    "    all_responses[task_id] = response\n",
    "    \n",
    "    answer_line = extract_response_after_question(response, problem['question'])\n",
    "\n",
    "    # Compare model output to the ground truth\n",
    "    model_number = extract_number_from_text(answer_line, \"The answer is\")\n",
    "    ground_truth_number = extract_number_from_text(problem['answer'], \"####\")\n",
    "    \n",
    "    # print(model_number)\n",
    "    # print(ground_truth_number)\n",
    "    \n",
    "    if model_number == ground_truth_number:\n",
    "        all_correct += 1\n",
    "    \n",
    "\n",
    "    print(f\"Model answer: {model_number}\")\n",
    "    print(f\"Ground truth answer: {ground_truth_number}\")\n",
    "    print(f\"Correct: {all_correct} out of {total}\")\n",
    "    print(\"=\"*40)\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "# Final accuracy\n",
    "accuracy = all_correct / len(gsm8k_test)\n",
    "print(f\"Final Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy Llama2 base : 14.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = all_correct / total* 100\n",
    "print(f\"Final Accuracy Llama2 base : {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
