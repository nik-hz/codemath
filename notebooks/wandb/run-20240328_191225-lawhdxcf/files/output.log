

  0%|▍                                                                                                    | 1/205 [00:50<2:52:42, 50.80s/it]

  1%|▉                                                                                                    | 2/205 [01:39<2:47:32, 49.52s/it]
{'loss': 2.3001, 'grad_norm': 7.404202461242676, 'learning_rate': 8e-05, 'epoch': 0.05}

  1%|█▍                                                                                                   | 3/205 [02:26<2:42:20, 48.22s/it]


  2%|██▍                                                                                                  | 5/205 [03:52<2:30:41, 45.21s/it]
  2%|██▍                                                                                                  | 5/205 [03:52<2:30:41, 45.21s/it]Traceback (most recent call last):
  File "/home/research/Columbia/codemath/notebooks/finetune_on_gsm8k.py", line 232, in <module>
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 360, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 359, in _fast_inner_training_loop
KeyboardInterrupt