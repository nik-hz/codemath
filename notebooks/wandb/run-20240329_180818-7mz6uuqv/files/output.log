
  0%|█▏                                                                                                                                                                                                                                          | 1/205 [00:54<3:05:35, 54.59s/it]Traceback (most recent call last):
  File "/home/research/Columbia/codemath/notebooks/finetune_on_gsm8k.py", line 240, in <module>
    trainer_stats = trainer.train()
                    ^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 360, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 422, in _fast_inner_training_loop
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 2412, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 3222, in evaluate
    eval_dataloader = self.get_eval_dataloader(eval_dataset)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 888, in get_eval_dataloader
    raise ValueError("Trainer: evaluation requires an eval_dataset.")
ValueError: Trainer: evaluation requires an eval_dataset.
{'loss': 1.5117, 'grad_norm': 1.129944920539856, 'learning_rate': 4e-05, 'epoch': 0.02}