

  0%|                                                                                                                                                                                                  | 1/8405 [00:02<6:56:14,  2.97s/it]
{'loss': 1.5987, 'grad_norm': 1.5928138494491577, 'learning_rate': 4e-05, 'epoch': 0.0}











































































  File "/home/research/Columbia/codemath/notebooks/finetune_on_gsm8k.py", line 249, in <module>███████████████████████████████████████████████████████████████████████████▌                               | 79/94 [08:47<03:12, 12.85s/it]
    trainer_stats = trainer.train()
                    ^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 360, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 422, in _fast_inner_training_loop
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 2412, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 3229, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer.py", line 3468, in evaluation_loop
    labels = nested_numpify(labels_host)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/research/.conda/envs/torch/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 156, in nested_numpify
    def nested_numpify(tensors):
KeyboardInterrupt