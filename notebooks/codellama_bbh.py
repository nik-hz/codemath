# -*- coding: utf-8 -*-
"""Codellama-BBH.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xD-fhLU-jk6e9Co8O1pa_IJVpiyrtrBZ
"""

from transformers import pipeline
import json

def load_dataset(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data["examples"]

# datasets
date_understanding = load_dataset('/content/drive/MyDrive/pretrain_data/date_understanding.json')
logical_deduction = load_dataset('/content/drive/MyDrive/pretrain_data/logical_deduction_seven_objects.json')
object_counting = load_dataset('/content/drive/MyDrive/pretrain_data/object_counting.json')

# Formatting the input with 5-shot prompting
def format_input(examples, task_description, shots=5):
    prompt = f"""<s>[INST] {task_description}\n\n"""
    for i in range(min(shots, len(examples))):
        example = examples[i]
        prompt += f"""### Input:\n{example['input']}\n### Response:\n{example['target']}\n"""
    prompt += "[/INST]</s>"
    return prompt


model = FastLanguageModel.from_pretrained('unsloth/codellama-34b-bnb-4bit')

# prediction function
def predict(model, input_text):
    inputs = tokenizer(input_text, return_tensors='pt', padding=True)
    outputs = model.generate(**inputs, max_length=50)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# testing function
def test_model(dataset, task_description):
    formatted_input = format_input(dataset, task_description)
    prediction = predict(model, formatted_input)
    print("Prediction:", prediction)

test_model(date_understanding, "Predict the correct date based on the input context.")
test_model(logical_deduction, "Deduce the correct order of finishing based on the descriptions.")
test_model(object_counting, "Count the number of relevant items based on the description.")