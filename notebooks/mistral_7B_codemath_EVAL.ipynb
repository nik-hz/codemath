{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral 7B Finetune CodeMath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training with the python script, we run evals in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=2048\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model_save_path = \"model_save_path/mistral_7b_finetuned_trace_python\"\n",
    "tokenizer_save_path = \"tokenizer_save_path/mistral_7b_finetuned_trace_python\"\n",
    "test_dataset_path = \"test.jsonl\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Traces\n",
    "\n",
    "Use BLEU to test the outputs of the model. Need some kind of custom testing or eval here later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "json_file_path = \"./python_states_singleline.json\"\n",
    "\n",
    "trace_prompt = \"\"\"<s>[INST] Below is an input which contains the state of variables and code that acts upon these variables or not. Given the state and the code give the state after the code executes for each variable. Be very careful. You should clearly outline your intermediate steps and your final answer should be a newline with exactly the variables and their values. Here is the State and Code. {}\n",
    "Now generate the final state for each variable. Generate intermediate outputs.[/INST] {}</s>\"\"\"\n",
    "\n",
    "trace_prompt2 = \"\"\"<s>[INST] Input: {}\n",
    "Now generate the final state for each variable. Generate intermediate outputs.[/INST]</s>\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        text = trace_prompt2.format(input)\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=json_file_path, split=\"train\")\n",
    "num_examples_to_select = 10\n",
    "subset_start = max(0, len(dataset) - num_examples_to_select)\n",
    "test_dataset = dataset.select(range(subset_start, len(dataset)))\n",
    "\n",
    "test_dataset_formatted = test_dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "# from transformers import pipeline\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# gen_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)#, device=0)  # Assuming using GPU 0\n",
    "\n",
    "# # Function to generate predictions for the test dataset\n",
    "# def generate_predictions(dataset, gen_pipeline):\n",
    "#     predictions = []\n",
    "#     # Use tqdm to add a progress bar\n",
    "#     for example in tqdm(dataset, desc=\"Generating predictions\"):\n",
    "#         # Generate text based on the input\n",
    "#         input_text = example['text']  # Ensure this matches your dataset structure\n",
    "#         generated_text = gen_pipeline(input_text, max_length=2048, num_return_sequences=1)[0]['generated_text']\n",
    "#         predictions.append(generated_text)\n",
    "#     return predictions\n",
    "\n",
    "# # Load the BLEU metric\n",
    "# bleu = load_metric(\"bleu\")\n",
    "\n",
    "# # Generate predictions for the formatted test dataset\n",
    "# predictions = generate_predictions(test_dataset_formatted, gen_pipeline)\n",
    "\n",
    "# # Prepare references in the format expected by the BLEU metric (a list of lists)\n",
    "# references = [[example['output'].split()] for example in test_dataset_formatted]  # Adjust based on your dataset structure\n",
    "\n",
    "# # Prepare predictions in the format expected by the BLEU metric\n",
    "# predictions_processed = [pred.split() for pred in predictions]\n",
    "\n",
    "# # Calculate BLEU score\n",
    "# results = bleu.compute(predictions=predictions_processed, references=references)\n",
    "\n",
    "# print(f\"BLEU score: {results['bleu'] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_and_compare(dataset, gen_pipeline):\n",
    "    for example in tqdm(dataset, desc=\"Generating predictions and comparing\"):\n",
    "        # Generate text based on the input\n",
    "        input_text = example['text']  # Adjust based on your dataset structure\n",
    "        generated_text = gen_pipeline(input_text, max_length=2048, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "        # Print input, expected output, and generated output for comparison\n",
    "        print(\"\\nInput Text:\\n\", input_text)\n",
    "        print(\"\\nExpected Output:\\n\", example['output'])\n",
    "        print(\"\\nGenerated Output:\\n\", generated_text)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "generate_predictions_and_compare(test_dataset_formatted, gen_pipeline)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
